{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyOqDzNpeWJeZl7Ia2X6ZlGg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"y4aSg0e1wvNb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761150068646,"user_tz":-480,"elapsed":17104,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}},"outputId":"149227e1-1b68-401b-af7a-f8ec57955d8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# google drive mounted\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["\n","### 1. Core Code\n","\n","```\n","MedSegDiff/\n","├── guided_diffusion/\n","│   ├── bratsloader.py              #BRATS数据加载器（可以和grade一起记录）\n","│   ├── medical_augmentation.py     #图像数据增强（没怎么做，cursor给我什么就是什么，交给你们了）\n","│   ├── losses.py                   # 损失函数（Dice, Joint, Focal等）\n","│   ├── evaluation_metrics.py       # 评估指标系统\n","│   ├── unet.py                     # UNet模型（包含CLS Head）\n","│   ├── gaussian_diffusion.py       # 扩散过程（包含多任务损失）\n","│   ├── train_util.py               # 训练工具（TrainLoop）\n","│   └── script_util.py              # 模型创建工具\n","├── scripts/\n","│   ├── segmentation_train.py       # 主训练脚本\n","│   ├── segmentation_sample.py        \n","└── requirement.txt                 # 依赖包列表\n","```\n","\n","### 2. Dataset Structure\n","\n","```\n","data/\n","├── BraTS2020/\n","│   └── training/\n","│       ├── slice0001/\n","│       │   ├── brats_train_001_t1_123_w.nii.gz\n","│       │   ├── brats_train_001_t1ce_123_w.nii.gz\n","│       │   ├── brats_train_001_t2_123_w.nii.gz\n","│       │   ├── brats_train_001_flair_123_w.nii.gz\n","│       │   └── brats_train_001_seg_123_w.nii.gz\n","│       └── slice0002/\n","│           └── ...\n","└── name_mapping.csv               # 病理分级标签文件\n","```\n"],"metadata":{"id":"qLOZ3br5VJ7e"}},{"cell_type":"code","source":["# 解压\n","# 从https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation下载training_data数据集及其附件（csv）\n","import zipfile, os\n","\n","zip_path = '/content/drive/MyDrive/Colab Notebooks/6204-2/Data/BraTS/MICCAI_BraTS2020.zip'\n","\n","assert os.path.exists(zip_path), 'Please update zip_path to your data location!'\n","with zipfile.ZipFile(zip_path, 'r') as zf:\n","    zf.extractall('/content/drive/MyDrive/Colab Notebooks/6204-2/Data/BraTS/')\n","print('✅ Data extracted to /6204-2/Data/BraTS/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2dqyusiVXzt","executionInfo":{"status":"ok","timestamp":1760943035048,"user_tz":-480,"elapsed":584536,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}},"outputId":"bef962b4-bf8f-4a52-c8ba-274dfaf58e46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Data extracted to /6204-2/Data/BraTS/\n"]}]},{"cell_type":"markdown","source":["数据集只需要训练集。一是测试集都不含mask和label，无法检验。二是我们的sample步骤，不需要很多的病例，可能四五个就够了（不同类别的比较好），然后其他全部是训练集，大小也是足够的。分集合的逻辑暂时还是用split ratio来分，其实直接抽出来作为测试集也是可以的，不用特意用代码分。"],"metadata":{"id":"N0oNt6My9dws"}},{"cell_type":"code","source":["# 配环境\n","!cd \"/content/drive/MyDrive/Colab Notebooks/6204-2\"\n","!pip install -r \"/content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dQKlLFtsYxLq","executionInfo":{"status":"ok","timestamp":1761150089455,"user_tz":-480,"elapsed":20795,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}},"outputId":"01c97a93-27ad-4f45-d9e5-813a9a7008bf","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (2.8.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 2)) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 3)) (2.2.2)\n","Requirement already satisfied: blobfile in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 4)) (3.1.0)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 5)) (5.3.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 6)) (4.12.0.88)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (0.25.2)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 8)) (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (3.10.0)\n","Collecting batchgenerators (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10))\n","  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting visdom (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11))\n","  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 12)) (1.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (3.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 3)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 3)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 3)) (2025.2)\n","Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 4)) (3.23.0)\n","Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from blobfile->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 4)) (2.5.0)\n","Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 4)) (5.4.0)\n","Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 5)) (25.0)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (1.16.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (2025.10.4)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 7)) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 8)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 8)) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 9)) (3.2.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from batchgenerators->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10)) (1.0.0)\n","Collecting unittest2 (from batchgenerators->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10))\n","  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (2.32.4)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.12/dist-packages (from visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (6.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (1.17.0)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.12/dist-packages (from visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (1.33)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.12/dist-packages (from visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (1.9.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 1)) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch->visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (3.0.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (3.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 11)) (2025.10.5)\n","Collecting argparse (from unittest2->batchgenerators->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10))\n","  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n","Collecting traceback2 (from unittest2->batchgenerators->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10))\n","  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting linecache2 (from traceback2->unittest2->batchgenerators->-r /content/drive/MyDrive/Colab Notebooks/6204-2/requirement.txt (line 10))\n","  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n","Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n","Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: batchgenerators, visdom\n","  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=ccf5ca85450c0a1f6e7b64fb561b259c99b9278b73c19e052ff0610fb7d85e5e\n","  Stored in directory: /root/.cache/pip/wheels/28/21/2b/7b25080f9f5847e6c3162b89d859d7cec9f3093158e56bd008\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=40ae52348d8e703f75bdbaf64510835b829ed67dbc185451bdd357412eb4ea50\n","  Stored in directory: /root/.cache/pip/wheels/37/6c/38/64eeaa310e325aacda723e6df1f79ab5e9f31ba195264e04a8\n","Successfully built batchgenerators visdom\n","Installing collected packages: linecache2, argparse, traceback2, visdom, unittest2, batchgenerators\n","Successfully installed argparse-1.4.0 batchgenerators-0.25.1 linecache2-1.0.0 traceback2-1.4.0 unittest2-1.1.0 visdom-0.2.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse"]},"id":"220a9cca7bfc418a8a1f470344cd86e9"}},"metadata":{}}]},{"cell_type":"code","source":["# 检查数据路径和内容\n","import os\n","\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/6204-2/Data/BraTS/MICCAI_BraTS2020_TrainingData'\n","csv_path = '/content/drive/MyDrive/Colab Notebooks/6204-2/Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv'\n","\n","print(f\"数据目录存在: {os.path.exists(data_dir)}\")\n","print(f\"CSV文件存在: {os.path.exists(csv_path)}\")\n","\n","if os.path.exists(data_dir):\n","    subdirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n","    print(f\"找到 {len(subdirs)} 个子目录\")\n","    if subdirs:\n","        print(f\"前3个子目录: {subdirs[:3]}\")\n","        first_dir = os.path.join(data_dir, subdirs[0])\n","        files = os.listdir(first_dir)\n","        print(f\"第一个子目录的文件: {files}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0DNJ3WxkzjQ","executionInfo":{"status":"ok","timestamp":1760980328646,"user_tz":-480,"elapsed":141,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}},"outputId":"84539a3d-91ef-41a9-ebf7-ae6537b94ce8","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["数据目录存在: True\n","CSV文件存在: True\n","找到 369 个子目录\n","前3个子目录: ['BraTS20_Training_001', 'BraTS20_Training_002', 'BraTS20_Training_003']\n","第一个子目录的文件: ['BraTS20_Training_001_flair.nii', 'BraTS20_Training_001_seg.nii', 'BraTS20_Training_001_t1.nii', 'BraTS20_Training_001_t1ce.nii', 'BraTS20_Training_001_t2.nii']\n"]}]},{"cell_type":"markdown","source":["对于数据不平衡，我用了classweighting和focalloss。但是感觉还是不够，这是数据集选择的问题，实在解决不了可以不管，因为分类的验证集其实学习的非常好了，哪怕有imbalance。"],"metadata":{"id":"SHtLJS6o8R0q"}},{"cell_type":"code","source":["# demo 已经可以训练，我是拿训练300步的权重测试的后面的sample任务\n","%cd \"/content/drive/MyDrive/Colab Notebooks/6204-2\"\n","! python scripts/segmentation_train.py \\\n","    --data_name BRATS \\\n","    --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \\\n","    --out_dir ./results \\\n","    --image_size 128 \\\n","    --num_channels 128 \\\n","    --num_res_blocks 1 \\\n","    --num_heads 1 \\\n","    --learn_sigma True \\\n","    --use_scale_shift_norm False \\\n","    --attention_resolutions 16 \\\n","    --diffusion_steps 1000 \\\n","    --noise_schedule linear \\\n","    --rescale_learned_sigmas False \\\n","    --rescale_timesteps False \\\n","    --lr 1e-5 \\\n","    --batch_size 8 \\\n","    --microbatch 2 \\\n","    --schedule_sampler uniform \\\n","    --save_interval 200 \\\n","    --log_interval 10 \\\n","    --use_fp16 False \\\n","    --use_cls_head True \\\n","    --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \\\n","    --focal_gamma 2.0 \\\n","    --seg_focal_gamma 1.5 \\\n","    --seg_focal_lambda 0.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ondMMCRKCDIJ","outputId":"ca2cf997-6fdd-4cef-d3b0-9c95352fb5ac","executionInfo":{"status":"ok","timestamp":1761152915887,"user_tz":-480,"elapsed":989717,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/6204-2\n","Logging to ./results_diffmore\n","creating data loader...\n","Loaded CSV with 369 entries\n","Grade mapping loaded: 369 subjects\n","Grade distribution: LGG=76, HGG=293\n","Class weights: LGG=2.428, HGG=0.630\n","Class weights calculated: tensor([2.4276, 0.6297])\n","Warning: Unexpected file format: W39_1998.09.19_Segm.nii\n","Warning: Skipping incomplete datapoint with missing sequences: ['seg']\n","Applied train split: 294 subjects selected from 368 total\n","============================================================\n","Dataset Statistics:\n","  Total samples: 369\n","  LGG samples: 76 (20.60%)\n","  HGG samples: 293 (79.40%)\n","  Alpha_LGG: 2.4276\n","  Alpha_HGG: 0.6297\n","============================================================\n","creating model and diffusion...\n","Classification head enabled in model\n","training...\n","---------------------------\n","| cls_acc      | 0.5      |\n","| cls_auc      | 1        |\n","| cls_f1       | 0        |\n","| grad_norm    | 1        |\n","| loss         | 1        |\n","| loss_cal     | 0.3      |\n","| loss_cal_q0  | 0.3      |\n","| loss_cls     | 0.244    |\n","| loss_diff    | 0.999    |\n","| loss_diff_q0 | 0.999    |\n","| loss_q0      | 1        |\n","| loss_seg     | 4        |\n","| loss_total   | 2.12     |\n","| param_norm   | 188      |\n","| samples      | 8        |\n","| seg_dice     | 4.42e-08 |\n","| seg_iou      | 4.42e-08 |\n","| sigma_cls    | 1        |\n","| sigma_seg    | 1        |\n","| step         | 0        |\n","---------------------------\n","saving model 0...\n","saving model 0.9999...\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n","  warnings.warn(  # warn only once\n","---------------------------\n","| cls_acc      | 0        |\n","| cls_auc      | nan      |\n","| cls_f1       | 0        |\n","| grad_norm    | 1        |\n","| loss         | 0.999    |\n","| loss_cal     | 0.294    |\n","| loss_cal_q0  | 0.309    |\n","| loss_cal_q1  | 0.282    |\n","| loss_cal_q2  | 0.276    |\n","| loss_cal_q3  | 0.302    |\n","| loss_cls     | 0.132    |\n","| loss_diff    | 0.995    |\n","| loss_diff_q0 | 0.993    |\n","| loss_diff_q1 | 0.991    |\n","| loss_diff_q2 | 0.994    |\n","| loss_diff_q3 | 0.999    |\n","| loss_q0      | 1        |\n","| loss_q1      | 0.994    |\n","| loss_q2      | 0.997    |\n","| loss_q3      | 1        |\n","| loss_seg     | 3.95     |\n","| loss_total   | 2.04     |\n","| param_norm   | 188      |\n","| samples      | 88       |\n","| seg_dice     | 0.0657   |\n","| seg_iou      | 0.034    |\n","| sigma_cls    | 1        |\n","| sigma_seg    | 1        |\n","| step         | 10       |\n","---------------------------\n","---------------------------\n","| cls_acc      | 0.5      |\n","| cls_auc      | nan      |\n","| cls_f1       | 0.667    |\n","| grad_norm    | 1        |\n","| loss         | 0.986    |\n","| loss_cal     | 0.288    |\n","| loss_cal_q0  | 0.298    |\n","| loss_cal_q1  | 0.266    |\n","| loss_cal_q2  | 0.324    |\n","| loss_cal_q3  | 0.287    |\n","| loss_cls     | 0.111    |\n","| loss_diff    | 0.982    |\n","| loss_diff_q0 | 0.989    |\n","| loss_diff_q1 | 0.984    |\n","| loss_diff_q2 | 0.979    |\n","| loss_diff_q3 | 0.98     |\n","| loss_q0      | 0.995    |\n","| loss_q1      | 0.986    |\n","| loss_q2      | 0.982    |\n","| loss_q3      | 0.985    |\n","| loss_seg     | 3.92     |\n","| loss_total   | 2.01     |\n","| param_norm   | 188      |\n","| samples      | 168      |\n","| seg_dice     | 7.24e-10 |\n","| seg_iou      | 7.24e-10 |\n","| sigma_cls    | 1        |\n","| sigma_seg    | 1        |\n","| step         | 20       |\n","---------------------------\n","---------------------------\n","| cls_acc      | 0.5      |\n","| cls_auc      | nan      |\n","| cls_f1       | 0.667    |\n","| grad_norm    | 1        |\n","| loss         | 0.99     |\n","| loss_cal     | 0.283    |\n","| loss_cal_q0  | 0.293    |\n","| loss_cal_q1  | 0.334    |\n","| loss_cal_q2  | 0.268    |\n","| loss_cal_q3  | 0.28     |\n","| loss_cls     | 0.122    |\n","| loss_diff    | 0.98     |\n","| loss_diff_q0 | 0.999    |\n","| loss_diff_q1 | 0.976    |\n","| loss_diff_q2 | 0.974    |\n","| loss_diff_q3 | 0.954    |\n","| loss_q0      | 1.02     |\n","| loss_q1      | 0.979    |\n","| loss_q2      | 0.977    |\n","| loss_q3      | 0.959    |\n","| loss_seg     | 3.79     |\n","| loss_total   | 1.96     |\n","| param_norm   | 188      |\n","| samples      | 248      |\n","| seg_dice     | 6.71e-10 |\n","| seg_iou      | 6.71e-10 |\n","| sigma_cls    | 1        |\n","| sigma_seg    | 1        |\n","| step         | 30       |\n","---------------------------\n","---------------------------\n","| cls_acc      | 0        |\n","| cls_auc      | nan      |\n","| cls_f1       | 0        |\n","| grad_norm    | 1        |\n","| loss         | 0.961    |\n","| loss_cal     | 0.289    |\n","| loss_cal_q0  | 0.274    |\n","| loss_cal_q1  | 0.292    |\n","| loss_cal_q2  | 0.292    |\n","| loss_cal_q3  | 0.295    |\n","| loss_cls     | 0.122    |\n","| loss_diff    | 0.957    |\n","| loss_diff_q0 | 0.972    |\n","| loss_diff_q1 | 0.955    |\n","| loss_diff_q2 | 0.957    |\n","| loss_diff_q3 | 0.95     |\n","| loss_q0      | 0.975    |\n","| loss_q1      | 0.958    |\n","| loss_q2      | 0.959    |\n","| loss_q3      | 0.954    |\n","| loss_seg     | 4.02     |\n","| loss_total   | 2.07     |\n","| param_norm   | 188      |\n","| samples      | 328      |\n","| seg_dice     | 0.0482   |\n","| seg_iou      | 0.0247   |\n","| sigma_cls    | 1        |\n","| sigma_seg    | 1        |\n","| step         | 40       |\n","---------------------------\n","[rank0]: Traceback (most recent call last):\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/scripts/segmentation_train.py\", line 185, in <module>\n","[rank0]:     main()\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/scripts/segmentation_train.py\", line 148, in main\n","[rank0]:     ).run_loop()\n","[rank0]:       ^^^^^^^^^^\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/guided_diffusion/train_util.py\", line 223, in run_loop\n","[rank0]:     batch, cond, grade_label, name = next(data_iter)\n","[rank0]:                                      ^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 734, in __next__\n","[rank0]:     data = self._next_data()\n","[rank0]:            ^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 790, in _next_data\n","[rank0]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n","[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n","[rank0]:             ~~~~~~~~~~~~^^^^^\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/guided_diffusion/bratsloader.py\", line 344, in __getitem__\n","[rank0]:     o = torch.tensor(nib_img.get_fdata())[:,:,slice_idx]\n","[rank0]:                      ^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/nibabel/dataobj_images.py\", line 374, in get_fdata\n","[rank0]:     data = np.asanyarray(self._dataobj, dtype=dtype)\n","[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/nibabel/arrayproxy.py\", line 454, in __array__\n","[rank0]:     arr = self._get_scaled(dtype=dtype, slicer=())\n","[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/nibabel/arrayproxy.py\", line 423, in _get_scaled\n","[rank0]:     scaled = scaled.astype(np.promote_types(scaled.dtype, dtype), copy=False)\n","[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]: KeyboardInterrupt\n","[rank0]:[W1022 17:08:34.522538962 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"markdown","source":["```\n","| loss          | 0.864 | 总损失 (不确定性加权后)\n","| loss_seg      | 2.14  | 分割损失 (扩散+校准)\n","| loss_cls      | 0.00775 | 分类损失 (Focal Loss)\n","| loss_total    | 1.07  | 总损失 (分割+分类)\n","| loss_diff     | 0.76  | 扩散损失 (MSE)\n","| loss_cal      | 0.0837 | 校准损失\n","| loss_cal_q0   | 0.0794 | 校准损失25%分位数\n","| loss_cal_q1   | 0.0778 | 校准损失50%分位数\n","| loss_cal_q3   | 0.0882 | 校准损失75%分位数\n","```\n","\n","#### **Segmentation**:\n","```\n","| seg_dice      | 0.141 | Dice\n","| seg_iou       | 0.0759 | IoU\n","```\n","\n","#### **Classification**:\n","```\n","| cls_acc       | 1     | Classification Accuracy\n","| cls_f1        | 1     | F1\n","| cls_auc       | nan   | AUC (0-1, 越高越好, nan表示只有一类)\n","```\n","\n","#### **不确定性参数**:\n","```\n","(***这个是cursor给我加的什么七七八八的权重，暂时没明白有没有用，不然删掉也可以）\n","| sigma_seg     | 1.03  | 分割任务的不确定性权重\n","| sigma_cls     | 1.03  | 分类任务的不确定性权重\n","```\n","\n","#### **Training**:\n","```\n","| step          | 200   | 当前训练步数\n","| samples       | 201   | 已处理的样本数\n","| grad_norm     | 44.7  | 梯度范数 (应该<10, 过高表示梯度爆炸)\n","| param_norm    | 187   | 参数范数\n","```\n","#### **Warning**:\n","- **loss**: 突然跳跃或NaN\n","- **grad_norm**: >50 梯度爆炸\n","- **cls_auc**: nan 表示只有一类样本\n","- **seg_dice**: <0.1 分割效果很差\n","\n","```bash\n","--lr 1e-4              # 学习率\n","--batch_size 4         # 批次大小\n","--diffusion_steps 1000 # 扩散步数\n","--focal_gamma 2.0      # Focal Loss的γ\n","--seg_focal_gamma 1.5  # 分割Focal Loss的γ\n","--seg_focal_lambda 0.5 # 分割Focal Loss的权重\n","```"],"metadata":{"id":"ZT0kaiiS5a8C"}},{"cell_type":"markdown","source":["分类头的注入，我认为有一定的问题，因为模型分级，不需要很复杂的网络信息。可能直接用最初进入深层网络前的图像预测，就挺好的。或者用最后传出的前一层，可能有更多的信息（包含多模态毕竟），但是直接注入到模型融合前，和seg loss一起迭代，不太行，还没改。"],"metadata":{"id":"1I8PAMfc8FOg"}},{"cell_type":"markdown","source":["1.  训练模型，输入mask和5个多模态。一个病例有5种多模态3D切片，每种155个。因此在训练的过程中，由于样本grade标签不平衡，又把切片混起来训练，会出现全是nan的情况，也就是随机抽到的绝大多数情况是一种标签。\n","在训练的过程当中，由于分类头的位置在早期和anchor融合前，因此分类的clsloss很容易在loss递减的过程中，占主导位置。进而我们的seg loss几乎没有办法进步，一直在强烈的震荡，而cls loss很快就收敛了。\n","\n","2. 目前想到能改进的地方，分类头只利用深层网络的信息来训练，并且在训练的过程中和seg任务分开做（可以做dual-encoder，dual UNet或者其他，有很多论文都有多模态这个问题，这个是多模态的通病了）\n","\n","3. 想到一个投机取巧的点子，先训练模型的分割能力，训练前500次，把分类头权重freeze，并且不参与loss的迭代，这样没有梯度的更新。之后再把分割部分freeze，单独训练分类头，仅需要一点步数。最后用很少的步数，把全部权重都结冻，微调一下权重。我还试过Dual-task UNet，效果也不太好。接下来可能会是一下我这个投机取巧的方法。"],"metadata":{"id":"A1SmsAeD6VLB"}},{"cell_type":"code","source":["# validation/sample, only target on valuable mask\n","%cd \"/content/drive/MyDrive/Colab Notebooks/6204-2\"\n","\n","! python scripts/segmentation_sample.py \\\n","  --data_name BRATS \\\n","  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \\\n","  --out_dir ./results/pic \\\n","  --image_size 128 \\\n","  --num_channels 128 \\\n","  --num_res_blocks 1 \\\n","  --num_heads 1 \\\n","  --learn_sigma True \\\n","  --use_scale_shift_norm False \\\n","  --attention_resolutions 32 \\\n","  --diffusion_steps 100 \\\n","  --noise_schedule linear \\\n","  --rescale_learned_sigmas False \\\n","  --rescale_timesteps False \\\n","  --batch_size 1 \\\n","  --use_ddim False \\\n","  --model_path ./results/savedmodel000300.pt \\\n","  --num_ensemble 2 \\\n","  --debug True \\\n","  --use_cls_head True \\\n","  --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \\\n","  --num_eval_cases 1"],"metadata":{"id":"WFWw7dbxd5RC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761153047882,"user_tz":-480,"elapsed":11936,"user":{"displayName":"zj Chen","userId":"09020784127522418201"}},"outputId":"f027c9a0-c269-450a-e67c-54477fe5b599"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/6204-2\n","Logging to ./results/pic\n","Loaded CSV with 369 entries\n","Grade mapping loaded: 369 subjects\n","Grade distribution: LGG=76, HGG=293\n","Class weights: LGG=2.428, HGG=0.630\n","Class weights calculated: tensor([2.4276, 0.6297])\n","Warning: Unexpected file format: W39_1998.09.19_Segm.nii\n","Warning: Skipping incomplete datapoint with missing sequences: ['seg']\n","Applied validation split: 74 subjects selected from 368 total\n","creating model and diffusion...\n","Loading model from ./results/savedmodel000300.pt\n","[rank0]: Traceback (most recent call last):\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/scripts/segmentation_sample.py\", line 405, in <module>\n","[rank0]:     main()\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/scripts/segmentation_sample.py\", line 128, in main\n","[rank0]:     state_dict = dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n","[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/content/drive/MyDrive/Colab Notebooks/6204-2/guided_diffusion/dist_util.py\", line 64, in load_state_dict\n","[rank0]:     with bf.BlobFile(path, \"rb\") as f:\n","[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/blobfile/_ops.py\", line 397, in BlobFile\n","[rank0]:     return default_context.BlobFile(\n","[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/blobfile/_context.py\", line 895, in BlobFile\n","[rank0]:     f = io.FileIO(path, mode=mode)\n","[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","[rank0]: FileNotFoundError: [Errno 2] No such file or directory: './results/savedmodel000300.pt'\n","[rank0]:[W1022 17:10:46.401689751 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"markdown","source":["训练随机抽取全部训练集切片，因为非常不平衡的数据集，其实不太好，可以考虑把空的切片过滤，然后对低类别的切片增强。测试集以病例为一个样本，检测该病例的所有有效切片，综合得到最后的诊断。这个是我自己改的。更跟我们的。Sample代码几乎没问题了，除了一些很具体的log和最后的metric。"],"metadata":{"id":"rice3K285zdb"}},{"cell_type":"code","source":[],"metadata":{"id":"-tPXE_mz5uEm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"E2vZbvPU4LXQ"}}]}