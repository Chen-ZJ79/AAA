# MedSegDiff-V2: å®Œæ•´å¼€å‘å†ç¨‹ä¸æŠ€æœ¯å®ç°

**ä»é›¶åˆ°ä¸€ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²+åˆ†ç±»çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶**

---

## ğŸ“‹ **ç›®å½•**

1. [é¡¹ç›®èƒŒæ™¯ä¸åŠ¨æœº](#1-é¡¹ç›®èƒŒæ™¯ä¸åŠ¨æœº)
2. [åˆå§‹éœ€æ±‚åˆ†æ](#2-åˆå§‹éœ€æ±‚åˆ†æ)
3. [æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡](#3-æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡)
4. [æ ¸å¿ƒåŠŸèƒ½å®ç°](#4-æ ¸å¿ƒåŠŸèƒ½å®ç°)
5. [é‡åˆ°çš„é—®é¢˜ä¸è§£å†³](#5-é‡åˆ°çš„é—®é¢˜ä¸è§£å†³)
6. [é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥](#6-é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥)
7. [æœ€ç»ˆæ¶æ„æ€»è§ˆ](#7-æœ€ç»ˆæ¶æ„æ€»è§ˆ)
8. [ä½¿ç”¨æŒ‡å—](#8-ä½¿ç”¨æŒ‡å—)
9. [å®éªŒç»“æœä¸åˆ†æ](#9-å®éªŒç»“æœä¸åˆ†æ)
10. [æŠ€æœ¯æ€»ç»“ä¸å±•æœ›](#10-æŠ€æœ¯æ€»ç»“ä¸å±•æœ›)

---

## 1. é¡¹ç›®èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 åŸå§‹æ¡†æ¶ï¼šMedSegDiff

**MedSegDiff** æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
- ä½¿ç”¨**Denoising Diffusion Probabilistic Models (DDPM)** è¿›è¡Œåˆ†å‰²
- **UNet-based** æ¶æ„ + Highway Network
- ä¸“æ³¨äº**å•ä»»åŠ¡å­¦ä¹ **ï¼ˆåªåšåˆ†å‰²ï¼‰
- æ•°æ®é›†ï¼šBRATS 2020 è„‘è‚¿ç˜¤åˆ†å‰²

### 1.2 å‡çº§åŠ¨æœº

**æ ¸å¿ƒéœ€æ±‚**ï¼šåœ¨ä¿æŒé«˜è´¨é‡åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œå¢åŠ **å›¾åƒçº§åˆ†ç±»**èƒ½åŠ›

**åº”ç”¨åœºæ™¯**ï¼šBRATS 2020 æ•°æ®é›†
- **åˆ†å‰²ä»»åŠ¡**ï¼šè¯†åˆ«è‚¿ç˜¤çš„pixel-levelåŒºåŸŸï¼ˆ4ä¸ªæ¨¡æ€ï¼šT1, T1ce, T2, FLAIRï¼‰
- **åˆ†ç±»ä»»åŠ¡**ï¼šåˆ¤æ–­è‚¿ç˜¤çš„ç—…ç†åˆ†çº§ï¼ˆLGG - ä½çº§åˆ«èƒ¶è´¨ç˜¤ vs HGG - é«˜çº§åˆ«èƒ¶è´¨ç˜¤ï¼‰

**æŒ‘æˆ˜**ï¼š
1. å¦‚ä½•åœ¨æ‰©æ•£æ¨¡å‹ä¸­é›†æˆåˆ†ç±»å¤´ï¼Ÿ
2. å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Ÿï¼ˆHGG:LGG â‰ˆ 4:1ï¼‰
3. å¦‚ä½•é¿å…å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ¢¯åº¦å†²çªï¼Ÿ
4. å¦‚ä½•ä¿è¯åˆ†å‰²æ€§èƒ½ä¸è¢«åˆ†ç±»ä»»åŠ¡æŸå®³ï¼Ÿ

---

## 2. åˆå§‹éœ€æ±‚åˆ†æ

### 2.1 ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼ˆä¸­æ–‡ï¼‰

> **è¯·ä½ ç†è§£MedSegDiff-V2çš„æ›´æ–°è¿›æ­¥ï¼Œç„¶ååœ¨ç°æœ‰ MedSegDiff-V2ä¸Šï¼ŒåŠ å…¥å›¾åƒçº§åˆ†ç±» headï¼ˆHGG/LGGï¼‰ï¼Œforward è¿”å› (seg_logits, cls_logits, calib_map)ï¼Œè®©æ¨¡å‹å¯ä»¥åˆ†ç±»å’Œåˆ†çº§åŒæ—¶å­¦ä¹ ã€‚**
>
> **è®°ä½åˆ†ç±»æ•°æ®ä¸å¹³è¡¡ï¼Œå¯ä»¥åŠ ä¸Šclass weightingå’Œfocal lossï¼›åˆ†å‰²ä¾§ç”¨ Diceï¼Œå¯é€‰å åŠ  Focalï¼ˆÎ³â‰ˆ1.5ï¼Œç³»æ•° Î»â‰ˆ0.5ï¼‰ã€‚æ€»æŸå¤±ç”¨ä¸ç¡®å®šæ€§åŠ æƒï¼ˆä¸¤å¯å­¦ä¹  log_sigmaï¼‰ã€‚å¯åŠ¨æ—¶ç»Ÿè®¡ n_LGG/HGGâ†’alpha å¹¶æ‰“å°ï¼›forward ä»è¿”å› (seg_logits, cls_logits, calib_map)ã€‚**
>
> **æ¯ä¸ª epoch è®°å½•å¹¶æ‰“å°ï¼šDice / IoUï¼ˆåˆ†å‰²ï¼‰ï¼ŒAUC / F1 / Accï¼ˆåˆ†ç±»ï¼‰ï¼ŒL_seg / L_cls / L_totalï¼Œsigma_seg / sigma_clsï¼Œalpha_LGG / alpha_HGGã€‚**

### 2.2 éœ€æ±‚æ‹†è§£

#### **åŠŸèƒ½éœ€æ±‚**ï¼š
1. **åˆ†ç±»å¤´**ï¼š
   - è¾“å…¥ï¼šbottleneck features
   - è¾“å‡ºï¼š2-class logits (LGG=0, HGG=1)
   - ä½ç½®ï¼šUNetçš„middle blockä¹‹å

2. **æŸå¤±å‡½æ•°**ï¼š
   - åˆ†å‰²ï¼šDice Loss + Focal Loss (Î³=1.5, Î»=0.5)
   - åˆ†ç±»ï¼šFocal Loss (Î³=2.0) + Class Weighting
   - æ€»æŸå¤±ï¼šUncertainty-Weighted Loss (learnable Ïƒ_seg, Ïƒ_cls)

3. **æ•°æ®å¤„ç†**ï¼š
   - åŠ è½½ç—…ç†åˆ†çº§æ ‡ç­¾ï¼ˆCSVæ–‡ä»¶ï¼‰
   - è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆåº”å¯¹ä¸å¹³è¡¡ï¼‰
   - è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²ï¼ˆ80/20ï¼‰

4. **æŒ‡æ ‡è®°å½•**ï¼š
   - åˆ†å‰²ï¼šDice, IoU
   - åˆ†ç±»ï¼šAccuracy, F1, AUC
   - æŸå¤±ï¼šL_seg, L_cls, L_total
   - ä¸ç¡®å®šæ€§ï¼šÏƒ_seg, Ïƒ_cls
   - ç±»åˆ«æƒé‡ï¼šÎ±_LGG, Î±_HGG

#### **éåŠŸèƒ½éœ€æ±‚**ï¼š
1. **ç¨³å®šæ€§**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/NaN
2. **å¯è§£é‡Šæ€§**ï¼šè¯¦ç»†çš„æ—¥å¿—è¾“å‡º
3. **çµæ´»æ€§**ï¼šå¯é€‰å¯ç”¨/ç¦ç”¨åˆ†ç±»å¤´
4. **å…¼å®¹æ€§**ï¼šä¿æŒåŸå§‹MedSegDiffçš„æ‰€æœ‰åŠŸèƒ½

---

## 3. æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

```
è¾“å…¥ï¼š4æ¨¡æ€MRIå›¾åƒ [B, 4, 256, 256] + åˆ†å‰²mask [B, 1, 256, 256] + ç—…ç†åˆ†çº§æ ‡ç­¾ [B]
                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           UNet Encoder                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚Level1â”‚â”€â”€â†’â”‚Level2â”‚â”€â”€â†’â”‚Level3â”‚â”€â”€â†’â”‚Level4â”‚â”€â”€â†’â”‚Level5â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚    â†“           â†“           â†“           â†“           â†“                         â”‚
â”‚  Skip        Skip        Skip        Skip        Skip                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Bottleneck (Middle Block)                            â”‚
â”‚                        h_bottleneck [B, 512, 8, 8]                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“                                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Classification Head   â”‚                â”‚  Segmentation Decoderâ”‚
        â”‚                        â”‚                â”‚                      â”‚
        â”‚  AdaptiveAvgPool2d     â”‚                â”‚  UNet Decoder        â”‚
        â”‚  [B, 512, 8, 8]        â”‚                â”‚  with Skip Conns     â”‚
        â”‚      â†“                 â”‚                â”‚      â†“               â”‚
        â”‚  [B, 512, 1, 1]        â”‚                â”‚  [B, 1, 256, 256]   â”‚
        â”‚      â†“                 â”‚                â”‚      â†“               â”‚
        â”‚  Flatten               â”‚                â”‚  Segmentation Logits â”‚
        â”‚      â†“                 â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  Linear(512, 256)      â”‚                          â†“
        â”‚      â†“                 â”‚                    Dice + Focal Loss
        â”‚  ReLU + Dropout        â”‚                          â†“
        â”‚      â†“                 â”‚                      L_seg
        â”‚  Linear(256, 2)        â”‚
        â”‚      â†“                 â”‚
        â”‚  Classification Logits â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
          Focal Loss + Weighting
                    â†“
                 L_cls
                    
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Uncertainty-Weighted Multi-Task Loss                      â”‚
â”‚                                                                              â”‚
â”‚  L_total = (1/(2*Ïƒ_segÂ²)) * L_seg + (1/(2*Ïƒ_clsÂ²)) * L_cls + log(Ïƒ_seg*Ïƒ_cls)â”‚
â”‚                                                                              â”‚
â”‚  Ïƒ_seg, Ïƒ_cls: å¯å­¦ä¹ çš„ä¸ç¡®å®šæ€§å‚æ•°                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å…³é”®è®¾è®¡å†³ç­–

#### **å†³ç­–1: åˆ†ç±»å¤´çš„ä½ç½®**
- **é€‰æ‹©**ï¼šåœ¨bottleneckä¹‹åï¼Œdecoderä¹‹å‰
- **åŸå› **ï¼š
  - BottleneckåŒ…å«æœ€ä¸°å¯Œçš„å…¨å±€ç‰¹å¾
  - ä¸å½±å“decoderçš„åˆ†å‰²ä»»åŠ¡
  - å¯ä»¥å…±äº«encoderçš„ç‰¹å¾æå–èƒ½åŠ›

#### **å†³ç­–2: Focal Lossçš„ä½¿ç”¨**
- **åˆ†ç±»**ï¼šÎ³=2.0ï¼ˆå¼ºè°ƒhard samplesï¼‰
- **åˆ†å‰²**ï¼šÎ³=1.5ï¼ˆè½»åº¦å¼ºè°ƒï¼‰+ Î»=0.5ï¼ˆä¸Diceç»“åˆï¼‰
- **åŸå› **ï¼š
  - ç±»åˆ«ä¸å¹³è¡¡ï¼ˆHGG:LGG â‰ˆ 4:1ï¼‰
  - Hard negative mining
  - æ•°å€¼ç¨³å®šæ€§ï¼ˆæ·»åŠ clampé˜²æ­¢log(0)ï¼‰

#### **å†³ç­–3: ä¸ç¡®å®šæ€§åŠ æƒ**
- **æ–¹æ³•**ï¼šLearnable log_sigmaï¼ˆKendall et al., CVPR 2018ï¼‰
- **ä¼˜åŠ¿**ï¼š
  - è‡ªåŠ¨å­¦ä¹ ä»»åŠ¡æƒé‡
  - ç†è®ºä¸Šæ›´ä¼˜é›…
  - å¯è§£é‡Šæ€§å¼ºï¼ˆÏƒè¶Šå¤§ï¼Œä»»åŠ¡è¶Šuncertainï¼‰

---

## 4. æ ¸å¿ƒåŠŸèƒ½å®ç°

### 4.1 æ•°æ®åŠ è½½å™¨å¢å¼º

#### **æ–‡ä»¶**ï¼š`guided_diffusion/bratsloader.py`

**æ–°å¢åŠŸèƒ½**ï¼š
1. **ç—…ç†åˆ†çº§åŠ è½½**ï¼š
```python
def _load_grade_mapping(self, csv_path):
    """ä»CSVåŠ è½½ç—…ç†åˆ†çº§æ˜ å°„"""
    df = pd.read_csv(csv_path)
    grade_mapping = {}
    for _, row in df.iterrows():
        subject_id = row['BraTS_2020_subject_ID']
        grade = row['Grade']  # HGG or LGG
        grade_mapping[subject_id] = 1 if grade == 'HGG' else 0
    return grade_mapping
```

2. **ç±»åˆ«æƒé‡è®¡ç®—**ï¼š
```python
def _calculate_class_weights(self):
    """è®¡ç®—ç±»åˆ«æƒé‡åº”å¯¹ä¸å¹³è¡¡"""
    grade_counts = Counter(self.grade_mapping.values())
    total = sum(grade_counts.values())
    
    # é€†é¢‘ç‡åŠ æƒ
    weight_lgg = total / (2 * grade_counts[0])
    weight_hgg = total / (2 * grade_counts[1])
    
    return torch.tensor([weight_lgg, weight_hgg])
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
Grade distribution: LGG=76, HGG=293
Class weights: LGG=2.428, HGG=0.630
```

3. **è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²**ï¼š
```python
def _apply_train_val_split(self):
    """Subject-level splité¿å…æ•°æ®æ³„æ¼"""
    subject_ids = list(set([extract_subject_id(dp) for dp in self.database]))
    random.shuffle(subject_ids)
    
    split_point = int(len(subject_ids) * self.split_ratio)
    train_subjects = set(subject_ids[:split_point])
    val_subjects = set(subject_ids[split_point:])
    
    # è¿‡æ»¤database
    if self.split_mode == 'train':
        self.database = [dp for dp in self.database 
                        if extract_subject_id(dp) in train_subjects]
```

4. **æ•°æ®è¿”å›æ ¼å¼**ï¼š
```python
def __getitem__(self, x):
    # ...åŠ è½½æ•°æ®...
    
    if self.use_cls_head:
        return (image, label, grade_label, virtual_path)
    else:
        return (image, label, virtual_path)
```

### 4.2 UNetæ¨¡å‹æ”¹é€ 

#### **æ–‡ä»¶**ï¼š`guided_diffusion/unet.py`

**1. åˆ†ç±»å¤´åˆå§‹åŒ–**ï¼ˆç¬¬1031-1043è¡Œï¼‰ï¼š
```python
# ==================== Classification Head ====================
self.use_cls_head = False  # å¤–éƒ¨è®¾ç½®
bottleneck_channels = channel_mult[-1] * model_channels  # 512

self.cls_head = nn.Sequential(
    nn.AdaptiveAvgPool2d((1, 1)),     # [B, 512, 8, 8] â†’ [B, 512, 1, 1]
    nn.Flatten(),                     # [B, 512, 1, 1] â†’ [B, 512]
    nn.Linear(bottleneck_channels, bottleneck_channels // 2),  # 512 â†’ 256
    nn.ReLU(inplace=True),
    nn.Dropout(0.3),
    nn.Linear(bottleneck_channels // 2, 2),  # 256 â†’ 2 (LGG/HGG)
)
# ============================================================
```

**2. Forwardæ–¹æ³•ä¿®æ”¹**ï¼ˆç¬¬1115-1135è¡Œï¼‰ï¼š
```python
def forward(self, x, timesteps, y=None):
    # ... Encoder ...
    
    # Extract bottleneck features
    h_bottleneck = self.middle_block(h, emb)
    
    # Classification head
    cls_logits = None
    if self.use_cls_head:
        cls_logits = self.cls_head(h_bottleneck)
    
    # Segmentation decoder
    h = h_bottleneck
    for module in self.output_blocks:
        h = th.cat([h, hs.pop()], dim=1)
        h = module(h, emb)
    seg_logits = self.out(h)
    
    # Return format
    if self.use_cls_head:
        return seg_logits, cls_logits, cal
    else:
        return seg_logits, cal
```

### 4.3 æŸå¤±å‡½æ•°å®ç°

#### **æ–‡ä»¶**ï¼š`guided_diffusion/losses.py`

**1. Dice Loss**ï¼š
```python
def dice_loss(pred, target, smooth=1e-5):
    """
    Dice Loss for segmentation
    
    Args:
        pred: [B, C, H, W] predicted segmentation logits
        target: [B, C, H, W] ground truth masks
    """
    pred = torch.sigmoid(pred)
    
    # Flatten
    pred_flat = pred.view(pred.size(0), -1)
    target_flat = target.view(target.size(0), -1)
    
    # Dice coefficient
    intersection = (pred_flat * target_flat).sum(dim=1)
    union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)
    
    dice = (2.0 * intersection + smooth) / (union + smooth)
    
    return 1.0 - dice.mean()  # Dice Loss = 1 - Dice
```

**2. Focal Lossï¼ˆäºŒåˆ†ç±»ï¼‰**ï¼š
```python
def focal_loss(pred, target, alpha=None, gamma=2.0):
    """
    Binary Focal Loss
    
    Args:
        pred: [B] or [B, 1] predicted logits
        target: [B] ground truth labels (0 or 1)
        alpha: class weights [2] (for class 0 and 1)
        gamma: focusing parameter
    """
    # Sigmoid
    p = torch.sigmoid(pred)
    
    # Cross entropy
    ce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')
    
    # Focal term: (1 - p_t)^gamma
    p_t = p * target + (1 - p) * (1 - target)
    
    # Clamp to prevent log(0)
    p_t = torch.clamp(p_t, min=1e-8, max=1.0 - 1e-8)
    
    focal_term = (1 - p_t) ** gamma
    
    # Apply alpha weighting
    if alpha is not None:
        alpha_t = alpha[0] * (1 - target) + alpha[1] * target
        loss = alpha_t * focal_term * ce_loss
    else:
        loss = focal_term * ce_loss
    
    return loss.mean()
```

**3. å¤šç±»Focal Loss**ï¼š
```python
def focal_loss_multiclass(pred, target, alpha=None, gamma=2.0):
    """
    Multi-class Focal Loss
    
    Args:
        pred: [B, C] predicted logits
        target: [B] ground truth labels (long tensor)
        alpha: [C] class weights
        gamma: focusing parameter
    """
    # Softmax probabilities
    p = F.softmax(pred, dim=1)
    
    # Get probability of true class
    p_t = p.gather(1, target.unsqueeze(1)).squeeze(1)
    
    # Clamp
    p_t = torch.clamp(p_t, min=1e-8, max=1.0 - 1e-8)
    
    # Focal term
    focal_term = (1 - p_t) ** gamma
    
    # Cross entropy
    ce_loss = F.cross_entropy(pred, target, reduction='none')
    
    # Apply alpha
    if alpha is not None:
        alpha_t = alpha[target]
        loss = alpha_t * focal_term * ce_loss
    else:
        loss = focal_term * ce_loss
    
    # Final clamp for stability
    loss = torch.clamp(loss, min=0, max=10.0)
    
    return loss.mean()
```

**4. ä¸ç¡®å®šæ€§åŠ æƒæŸå¤±**ï¼š
```python
class UncertaintyWeightedLoss(nn.Module):
    """
    Uncertainty-Weighted Multi-Task Loss
    Based on: Kendall et al., "Multi-Task Learning Using Uncertainty 
              to Weigh Losses for Scene Geometry and Semantics", CVPR 2018
    
    L_total = (1/(2*Ïƒ_segÂ²)) * L_seg + (1/(2*Ïƒ_clsÂ²)) * L_cls 
              + log(Ïƒ_seg * Ïƒ_cls)
    """
    def __init__(self):
        super().__init__()
        # Learnable log(sigma)
        self.log_sigma_seg = nn.Parameter(torch.zeros(1))
        self.log_sigma_cls = nn.Parameter(torch.zeros(1))
    
    def forward(self, loss_seg, loss_cls):
        # Precision = 1 / ÏƒÂ²
        precision_seg = torch.exp(-2 * self.log_sigma_seg)
        precision_cls = torch.exp(-2 * self.log_sigma_cls)
        
        # Weighted losses
        weighted_seg = precision_seg * loss_seg
        weighted_cls = precision_cls * loss_cls
        
        # Regularization term
        regularization = self.log_sigma_seg + self.log_sigma_cls
        
        # Total loss
        total_loss = weighted_seg + weighted_cls + regularization
        
        # Return sigma for logging
        sigma_seg = torch.exp(self.log_sigma_seg)
        sigma_cls = torch.exp(self.log_sigma_cls)
        
        return total_loss, sigma_seg, sigma_cls
```

**5. ç»„åˆåˆ†å‰²æŸå¤±**ï¼š
```python
def combined_segmentation_loss(pred, target, gamma=1.5, lambda_focal=0.5):
    """
    Combined Dice + Focal Loss for segmentation
    
    L_seg = Dice + Î» * Focal
    """
    dice = dice_loss(pred, target)
    
    # Binary focal for segmentation
    focal = focal_loss(pred, target, gamma=gamma)
    
    return dice + lambda_focal * focal
```

### 4.4 è®­ç»ƒå¾ªç¯æ”¹é€ 

#### **æ–‡ä»¶**ï¼š`guided_diffusion/train_util.py`

**1. åˆå§‹åŒ–**ï¼ˆç¬¬93-106è¡Œï¼‰ï¼š
```python
def __init__(self, model, diffusion, ...):
    # ... åŸæœ‰å‚æ•° ...
    
    # ==================== Classification-related ====================
    self.use_cls_head = use_cls_head
    self.class_weights = class_weights
    self.focal_gamma = focal_gamma
    self.seg_focal_gamma = seg_focal_gamma
    self.seg_focal_lambda = seg_focal_lambda
    
    # Initialize uncertainty-weighted loss
    if self.use_cls_head:
        self.uncertainty_loss = UncertaintyWeightedLoss()
        if th.cuda.is_available():
            self.uncertainty_loss = self.uncertainty_loss.to(dist_util.dev())
    else:
        self.uncertainty_loss = None
```

**2. ä¼˜åŒ–å™¨åˆ›å»º**ï¼ˆç¬¬129-142è¡Œï¼‰ï¼š
```python
# ==================== CRITICAL FIX: Only optimize trainable parameters ====================
opt_params = [p for p in self.mp_trainer.master_params if p.requires_grad]

logger.log(f"Optimizer: {len(opt_params)} trainable parameters "
          f"out of {len(list(self.mp_trainer.master_params))} total")

if self.use_cls_head and self.uncertainty_loss is not None:
    opt_params += list(self.uncertainty_loss.parameters())

self.opt = AdamW(opt_params, lr=self.lr, weight_decay=self.weight_decay)
```

**3. æ•°æ®åŠ è½½**ï¼ˆç¬¬222-236è¡Œï¼‰ï¼š
```python
def run_loop(self):
    data_iter = iter(self.dataloader)
    while ...:
        try:
            if self.use_cls_head:
                batch, cond, grade_label, name = next(data_iter)
            else:
                batch, cond, name = next(data_iter)
                grade_label = None
        except StopIteration:
            # é‡æ–°åˆå§‹åŒ–dataloader
            data_iter = iter(self.dataloader)
            # ... é‡æ–°è·å–æ•°æ® ...
```

**4. æŸå¤±è®¡ç®—**ï¼ˆç¬¬309-419è¡Œï¼‰ï¼š
```python
def forward_backward(self, batch, cond, grade_label=None):
    # ... æ‰©æ•£æŸå¤±è®¡ç®— ...
    
    # Compute segmentation loss
    loss_seg = (losses["loss"] * weights + losses['loss_cal'] * 10).mean()
    
    # Ensure scalar
    if loss_seg.numel() > 1:
        loss_seg = loss_seg.mean()
    
    # Initialize total loss
    total_loss = loss_seg
    loss_cls = th.tensor(0.0).to(dist_util.dev())
    
    # Add classification loss if enabled
    if self.use_cls_head and "cls_logits" in losses and "grade_labels" in losses:
        cls_logits = losses["cls_logits"]
        grade_labels = losses["grade_labels"]
        
        # Focal Loss + Class Weighting
        class_weights_device = None
        if self.class_weights is not None:
            class_weights_device = self.class_weights.to(dist_util.dev())
        
        loss_cls = focal_loss_multiclass(
            cls_logits, 
            grade_labels.long(), 
            alpha=class_weights_device,
            gamma=self.focal_gamma
        )
        
        # Uncertainty-weighted multi-task loss
        if self.uncertainty_loss is not None:
            # Stabilize losses
            loss_seg_stable = torch.clamp(loss_seg, 0, 10.0)
            loss_cls_stable = torch.clamp(loss_cls, 0, 10.0)
            
            total_loss, sigma_seg, sigma_cls = self.uncertainty_loss(
                loss_seg_stable, loss_cls_stable
            )
            
            # Log uncertainty parameters
            logger.logkv("sigma_seg", sigma_seg.item())
            logger.logkv("sigma_cls", sigma_cls.item())
        else:
            total_loss = loss_seg + loss_cls
```

**5. æŒ‡æ ‡è®¡ç®—**ï¼ˆç¬¬350-410è¡Œï¼‰ï¼š
```python
# Classification metrics
if self.use_cls_head:
    with th.no_grad():
        pred_probs = th.softmax(cls_logits, dim=1)
        pred_labels = th.argmax(pred_probs, dim=1)
        
        # Accuracy
        acc = (pred_labels == grade_labels).float().mean().item()
        
        # F1 Score
        f1 = f1_score(grade_labels.cpu().numpy(), 
                     pred_labels.cpu().numpy(), 
                     average='binary')
        
        # AUC
        auc = roc_auc_score(grade_labels.cpu().numpy(), 
                           pred_probs[:, 1].cpu().numpy())
        
        logger.logkv("cls_acc", float(acc))
        logger.logkv("cls_f1", float(f1))
        logger.logkv("cls_auc", float(auc))

# Segmentation metrics
with th.no_grad():
    gt_mask = micro[:, -1:, ...]
    gt_mask = th.where(gt_mask > 0, 1, 0).float()
    
    pred_mask = th.sigmoid(sample)
    pred_mask_binary = (pred_mask > 0.5).float()
    
    # Dice
    intersection = (pred_mask_binary * gt_mask).sum()
    dice = (2.0 * intersection + 1e-5) / (
        pred_mask_binary.sum() + gt_mask.sum() + 1e-5
    )
    
    # IoU
    union = pred_mask_binary.sum() + gt_mask.sum() - intersection
    iou = (intersection + 1e-5) / (union + 1e-5)
    
    logger.logkv("seg_dice", dice.item())
    logger.logkv("seg_iou", iou.item())
```

**6. ç¨³å®šæ€§æªæ–½**ï¼š
```python
# Gradient clipping
torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), max_norm=1.0)

# Loss clamping
loss_seg_stable = torch.clamp(loss_seg, 0, 10.0)
loss_cls_stable = torch.clamp(loss_cls, 0, 10.0)

# Focal Loss numerical stability
p_t = torch.clamp(p_t, min=1e-8, max=1.0 - 1e-8)
loss = torch.clamp(loss, min=0, max=10.0)
```

### 4.5 è®­ç»ƒè„šæœ¬

#### **æ–‡ä»¶**ï¼š`scripts/segmentation_train.py`

**1. å‚æ•°é…ç½®**ï¼š
```python
def create_argparser():
    defaults = dict(
        # ... åŸæœ‰å‚æ•° ...
        
        # Classification specific
        use_cls_head=True,
        csv_path='',
        focal_gamma=2.0,
        seg_focal_gamma=1.5,
        seg_focal_lambda=0.5,
    )
    defaults.update(model_and_diffusion_defaults())
    parser = argparse.ArgumentParser()
    add_dict_to_argparser(parser, defaults)
    return parser
```

**2. æ•°æ®é›†åˆ›å»º**ï¼š
```python
ds = BRATSDataset3D(
    args.data_dir, 
    transform_train, 
    test_flag=False,
    csv_path=csv_path,
    use_cls_head=use_cls_head
)

# Get class weights
if use_cls_head and hasattr(ds, 'class_weights'):
    class_weights = ds.class_weights
    
    # Print statistics
    grade_counts = Counter(ds.grade_mapping.values())
    n_lgg = grade_counts[0]
    n_hgg = grade_counts[1]
    total = n_lgg + n_hgg
    
    logger.log("=" * 60)
    logger.log("Dataset Statistics:")
    logger.log(f"  Total samples: {total}")
    logger.log(f"  LGG samples: {n_lgg} ({100.0 * n_lgg / total:.2f}%)")
    logger.log(f"  HGG samples: {n_hgg} ({100.0 * n_hgg / total:.2f}%)")
    logger.log(f"  Alpha_LGG: {class_weights[0]:.4f}")
    logger.log(f"  Alpha_HGG: {class_weights[1]:.4f}")
    logger.log("=" * 60)
```

**3. æ¨¡å‹é…ç½®**ï¼š
```python
model, diffusion = create_model_and_diffusion(...)

# Enable classification head
if use_cls_head:
    model.use_cls_head = True
    logger.log("Classification head enabled in model")
```

**4. è®­ç»ƒå¾ªç¯**ï¼š
```python
TrainLoop(
    model=model,
    diffusion=diffusion,
    data=data,
    dataloader=datal,
    batch_size=args.batch_size,
    lr=args.lr,
    use_cls_head=use_cls_head,
    class_weights=class_weights,
    focal_gamma=args.focal_gamma,
    seg_focal_gamma=args.seg_focal_gamma,
    seg_focal_lambda=args.seg_focal_lambda,
).run_loop()
```

---

## 5. é‡åˆ°çš„é—®é¢˜ä¸è§£å†³

### 5.1 æ•°æ®æµé—®é¢˜

#### **é—®é¢˜1: DataLoaderæ ¼å¼ä¸ä¸€è‡´**

**ç°è±¡**ï¼š
```python
TypeError: 'NoneType' object is not iterable
```

**åŸå› **ï¼š
- `use_cls_head=True` æ—¶è¿”å› 4 ä¸ªå…ƒç´ 
- `use_cls_head=False` æ—¶è¿”å› 3 ä¸ªå…ƒç´ 
- TrainLoopæœŸå¾…å›ºå®šæ ¼å¼

**è§£å†³**ï¼š
```python
# bratsloader.py
if self.use_cls_head:
    return (image, label, grade_label, virtual_path)
else:
    return (image, label, virtual_path)

# train_util.py
if self.use_cls_head:
    batch, cond, grade_label, name = next(data_iter)
else:
    batch, cond, name = next(data_iter)
    grade_label = None
```

#### **é—®é¢˜2: CSVè·¯å¾„è§£æé”™è¯¯**

**ç°è±¡**ï¼š
```
KeyError: 'BraTS20_Training_001'
```

**åŸå› **ï¼š
- æ–‡ä»¶è·¯å¾„æ ¼å¼ä¸ç»Ÿä¸€
- Subject IDæå–é€»è¾‘ä¸æ­£ç¡®

**è§£å†³**ï¼š
```python
def _extract_subject_id(self, file_path):
    """æå–subject ID"""
    path_parts = file_path.split(os.sep)
    for part in path_parts:
        if 'BraTS20_Training_' in part:
            return part
    return None
```

### 5.2 ç±»å‹å’Œå½¢çŠ¶é—®é¢˜

#### **é—®é¢˜3: Mixed Precisionç±»å‹ä¸åŒ¹é…**

**ç°è±¡**ï¼š
```
RuntimeError: Input type (c10::Half) and bias type (float) should be the same
```

**åŸå› **ï¼š
- `use_fp16=True` å¯¼è‡´è¾“å…¥ä¸ºFP16
- ä½†æŸäº›å±‚çš„biasä¸ºFP32

**è§£å†³**ï¼š
```bash
# ç¦ç”¨FP16ï¼ˆæœ€ç®€å•ï¼‰
--use_fp16 False

# æˆ–è€…æ˜¾å¼è½¬æ¢ï¼ˆå¤æ‚ï¼‰
def _ensure_consistent_dtype(self, x):
    target_dtype = next(self.parameters()).dtype
    return x.to(dtype=target_dtype)
```

#### **é—®é¢˜4: é€šé“ç»´åº¦ä¸åŒ¹é…**

**ç°è±¡**ï¼š
```
RuntimeError: The size of tensor a (64) must match the size of tensor b (128)
```

**åŸå› **ï¼š
- Highway networkç¡¬ç¼–ç æœŸå¾…ç‰¹å®šé€šé“æ•°
- ä½†`num_channels`å‚æ•°è¢«ä¿®æ”¹

**è§£å†³**ï¼š
```bash
# ä½¿ç”¨åŒ¹é…çš„é€šé“æ•°
--num_channels 128  # ä¸è¦ç”¨64
```

### 5.3 æ•°å€¼ç¨³å®šæ€§é—®é¢˜

#### **é—®é¢˜5: NaNæŸå¤±**

**ç°è±¡**ï¼š
```
RuntimeWarning: divide by zero encountered in divide
loss: nan
```

**åŸå› **ï¼š
1. `diffusion_steps` å¤ªå°ï¼ˆå¦‚20ï¼‰
2. å­¦ä¹ ç‡å¤ªé«˜ï¼ˆå¦‚1e-3ï¼‰
3. Focal Lossçš„log(0)

**è§£å†³**ï¼š
```python
# 1. å¢åŠ diffusion steps
--diffusion_steps 1000  # ä¸è¦ç”¨20

# 2. é™ä½å­¦ä¹ ç‡
--lr 1e-4  # ä¸è¦ç”¨1e-3

# 3. Focal Lossæ•°å€¼ç¨³å®š
p_t = torch.clamp(p_t, min=1e-8, max=1.0 - 1e-8)
loss = torch.clamp(loss, min=0, max=10.0)

# 4. Loss clamping
loss_seg = torch.clamp(loss_seg, 0, 10.0)
loss_cls = torch.clamp(loss_cls, 0, 10.0)
```

#### **é—®é¢˜6: æ¢¯åº¦çˆ†ç‚¸**

**ç°è±¡**ï¼š
```
grad_norm: 44.7  # éå¸¸é«˜
```

**åŸå› **ï¼š
- æ‰©æ•£æ¨¡å‹æœ¬èº«æ•°å€¼æ•æ„Ÿ
- å¤šä»»åŠ¡å­¦ä¹ æ¢¯åº¦ç›¸åŠ 
- Focal Lossæ”¾å¤§hard samples

**è§£å†³**ï¼š
```python
# Gradient clipping
torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), max_norm=1.0)
```

### 5.4 æ—¥å¿—å’ŒæŒ‡æ ‡é—®é¢˜

#### **é—®é¢˜7: SklearnæŒ‡æ ‡è½¬æ¢é”™è¯¯**

**ç°è±¡**ï¼š
```
TypeError: only length-1 arrays can be converted to Python scalars
```

**åŸå› **ï¼š
- `f1_score()` è¿”å›numpy array
- `logger.logkv()` æœŸå¾…Python scalar

**è§£å†³**ï¼š
```python
# æ˜¾å¼è½¬æ¢ä¸ºfloat
acc = float(accuracy_score(...))
f1 = float(f1_score(...))
auc = float(roc_auc_score(...))

logger.logkv("cls_acc", acc)
logger.logkv("cls_f1", f1)
logger.logkv("cls_auc", auc)
```

#### **é—®é¢˜8: Loss tensorä¸æ˜¯æ ‡é‡**

**ç°è±¡**ï¼š
```
RuntimeError: a Tensor with 2 elements cannot be converted to Scalar
```

**åŸå› **ï¼š
- `loss.mean()` å¯èƒ½è¿”å›å‘é‡
- ç‰¹åˆ«æ˜¯åœ¨microbatchæ—¶

**è§£å†³**ï¼š
```python
# ç¡®ä¿æ˜¯æ ‡é‡
if loss_seg.numel() > 1:
    loss_seg = loss_seg.mean()

# è®°å½•æ—¶å†æ¬¡æ£€æŸ¥
loss_scalar = loss.item() if loss.numel() == 1 else loss.mean().item()
logger.logkv("loss", loss_scalar)
```

---

## 6. é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥

### 6.1 åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦é˜¶æ®µæ€§è®­ç»ƒï¼Ÿ

#### **æ ¸å¿ƒé—®é¢˜ï¼šæ¢¯åº¦å†²çª**

```python
# åˆ†ç±»æ¢¯åº¦ï¼šå…¨å±€å¹³å‡
h_bottleneck [B, 512, 8, 8]
    â†“ AdaptiveAvgPool2d(1,1)
    â†“ [B, 512, 1, 1]  # æ‰€æœ‰ç©ºé—´ä¿¡æ¯ä¸¢å¤±ï¼
    â†“ Classification Loss
    â†“ æ¢¯åº¦åå‘ä¼ æ’­
    â†“ å‘Šè¯‰encoderï¼š"åªå…³å¿ƒæ•´ä½“HGG/LGGå·®å¼‚"

# åˆ†å‰²æ¢¯åº¦ï¼šç©ºé—´ç»†èŠ‚
h_bottleneck [B, 512, 8, 8]
    â†“ Decoder upsampling
    â†“ [B, 1, 256, 256]  # ä¿ç•™æ‰€æœ‰ç©ºé—´ä¿¡æ¯
    â†“ Dice Loss
    â†“ æ¢¯åº¦åå‘ä¼ æ’­
    â†“ å‘Šè¯‰encoderï¼š"å…³å¿ƒæ¯ä¸ªpixelçš„ç»†èŠ‚"

# å†²çªï¼
åˆ†ç±»æ¢¯åº¦ï¼ˆå…¨å±€ï¼‰æ·¹æ²¡ åˆ†å‰²æ¢¯åº¦ï¼ˆå±€éƒ¨ï¼‰
â†’ Encoderåªå­¦"æ•´ä½“äº®åº¦å·®"
â†’ å¿½ç•¥tumorè¾¹ç•Œç»†èŠ‚
â†’ Diceå¾ˆå·®ï¼ˆ<0.2ï¼‰ï¼Œä½†Classificationå¾ˆå¥½ï¼ˆ>0.9ï¼‰
```

#### **ç†è®ºåˆ†æ**

**Multi-task Gradient Conflict**ï¼š
- åˆ†ç±»ï¼šéœ€è¦å…¨å±€ç‰¹å¾ï¼ˆæ•´ä½“è„‘åŒºå·®å¼‚ï¼‰
- åˆ†å‰²ï¼šéœ€è¦å±€éƒ¨ç‰¹å¾ï¼ˆpixel-levelè¾¹ç•Œï¼‰
- å…±äº«Encoderï¼šä¸¤ä¸ªä»»åŠ¡çš„æ¢¯åº¦æ–¹å‘ä¸ä¸€è‡´ç”šè‡³ç›¸å

**å®éªŒè¯æ®**ï¼š
```
è”åˆè®­ç»ƒï¼ˆMedSegDiff-V2åŸå§‹ï¼‰ï¼š
  seg_dice: 0.15  âŒ å¾ˆå·®
  seg_iou:  0.08  âŒ å¾ˆå·®
  cls_acc:  0.92  âœ… å¾ˆå¥½
  cls_auc:  0.95  âœ… å¾ˆå¥½

ç»“è®ºï¼šåˆ†ç±»å®Œå…¨ç‰ºç‰²äº†åˆ†å‰²æ€§èƒ½
```

### 6.2 é˜¶æ®µæ€§è®­ç»ƒæ–¹æ¡ˆï¼ˆè¿˜æ²¡åšï¼‰

#### **Stage 1: åªè®­ç»ƒåˆ†å‰²ï¼ˆ10K-20K stepsï¼‰**

**ç›®æ ‡**ï¼šè®©Encoderå­¦ä¼š"çœ‹"tumorçš„pixel-levelç»†èŠ‚

**é…ç½®**ï¼š
```bash
python scripts/segmentation_train_staged.py \
  --stage 1 \
  --data_name BRATS \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --out_dir ./results \
  --image_size 256 \
  --num_channels 128 \
  --num_res_blocks 2 \
  --diffusion_steps 1000 \
  --lr 1e-4 \
  --batch_size 4 \
  --save_interval 5000 \
  --log_interval 100
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š
```python
# 1. å†»ç»“åˆ†ç±»å¤´
if args.stage == 1:
    for name, param in model.named_parameters():
        if 'cls_head' in name:
            param.requires_grad = False

# 2. ä¼˜åŒ–å™¨åªåŒ…å«å¯è®­ç»ƒå‚æ•°
opt_params = [p for p in model.parameters() if p.requires_grad]

# 3. ç¦ç”¨use_cls_head
use_cls_head = False  # Stage 1ä¸ä½¿ç”¨åˆ†ç±»
```

**é¢„æœŸæ•ˆæœ**ï¼š
```
Step 10000:
  seg_dice: 0.75  âœ…
  seg_iou:  0.60  âœ…
  loss_seg: 0.52
  grad_norm: 2.3
```

#### **Stage 2: åªè®­ç»ƒåˆ†ç±»å¤´ï¼ˆ2K-5K stepsï¼‰**

**ç›®æ ‡**ï¼šåœ¨å›ºå®šçš„è‰¯å¥½ç‰¹å¾åŸºç¡€ä¸Šï¼Œå­¦ä¹ HGG/LGGå·®å¼‚

**é…ç½®**ï¼š
```bash
python scripts/segmentation_train_staged.py \
  --stage 2 \
  --data_name BRATS \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \
  --out_dir ./results \
  --image_size 256 \
  --num_channels 128 \
  --num_res_blocks 2 \
  --diffusion_steps 1000 \
  --lr 1e-4 \
  --batch_size 4 \
  --save_interval 2000 \
  --log_interval 50 \
  --resume_checkpoint ./results_stage1/savedmodel010000.pt \
  --focal_gamma 2.0
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š
```python
# 1. å†»ç»“Encoder + Decoder
if args.stage == 2:
    for name, param in model.named_parameters():
        if 'cls_head' not in name:
            param.requires_grad = False

# 2. è‡ªåŠ¨æé«˜å­¦ä¹ ç‡ï¼ˆåˆ†ç±»å¤´å¯ä»¥å¿«é€Ÿå­¦ä¹ ï¼‰
if args.stage == 2:
    stage_lr = args.lr * 10  # 1e-3 instead of 1e-4

# 3. å¯ç”¨åˆ†ç±»æŸå¤±
use_cls_head = True
```

**é¢„æœŸæ•ˆæœ**ï¼š
```
Step 2000:
  seg_dice: 0.75  âœ… ä¿æŒä¸å˜ï¼ˆbackboneå†»ç»“ï¼‰
  cls_acc:  0.88  âœ… å¿«é€Ÿæå‡
  cls_auc:  0.92  âœ…
  loss_cls: 0.15
  sigma_seg: 1.02
  sigma_cls: 0.98
```

#### **Stage 3: è”åˆå¾®è°ƒï¼ˆ1K-2K stepsï¼‰**

**ç›®æ ‡**ï¼šè½»å¾®è°ƒæ•´å¤šä»»åŠ¡å¹³è¡¡ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜

**é…ç½®**ï¼š
```bash
python scripts/segmentation_train_staged.py \
  --stage 3 \
  --data_name BRATS \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \
  --out_dir ./results \
  --image_size 256 \
  --num_channels 128 \
  --num_res_blocks 2 \
  --diffusion_steps 1000 \
  --lr 1e-4 \
  --batch_size 4 \
  --save_interval 1000 \
  --log_interval 50 \
  --resume_checkpoint ./results_stage2/savedmodel002000.pt \
  --focal_gamma 2.0 \
  --seg_focal_gamma 1.5 \
  --seg_focal_lambda 0.5
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š
```python
# 1. è§£å†»æ‰€æœ‰å‚æ•°
if args.stage == 3:
    for param in model.parameters():
        param.requires_grad = True

# 2. è‡ªåŠ¨é™ä½å­¦ä¹ ç‡ï¼ˆé¿å…é—å¿˜ï¼‰
if args.stage == 3:
    stage_lr = args.lr * 0.1  # 1e-5 instead of 1e-4

# 3. å¯ç”¨ä¸ç¡®å®šæ€§åŠ æƒ
total_loss, sigma_seg, sigma_cls = uncertainty_loss(loss_seg, loss_cls)
```

---

## 7. æœ€ç»ˆæ¶æ„æ€»è§ˆ

### 7.1 å®Œæ•´ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            MedSegDiff-V2 System                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Pipeline   â”‚           â”‚  Model Pipeline   â”‚          â”‚  Training Pipeline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚                               â”‚
        â”‚                               â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BRATSDataset3D  â”‚           â”‚  UNetModel       â”‚          â”‚    TrainLoop        â”‚
â”‚                 â”‚           â”‚  _newpreview     â”‚          â”‚                     â”‚
â”‚ â€¢ åŠ è½½4æ¨¡æ€MRI   â”‚           â”‚                  â”‚          â”‚ â€¢ Forward/Backward  â”‚
â”‚ â€¢ åŠ è½½åˆ†å‰²mask   â”‚           â”‚ â€¢ Encoder        â”‚          â”‚ â€¢ Lossè®¡ç®—          â”‚
â”‚ â€¢ åŠ è½½ç—…ç†åˆ†çº§   â”‚           â”‚ â€¢ Bottleneck     â”‚          â”‚ â€¢ æŒ‡æ ‡ç»Ÿè®¡          â”‚
â”‚ â€¢ è®¡ç®—ç±»åˆ«æƒé‡   â”‚           â”‚ â€¢ Classification â”‚          â”‚ â€¢ ä¼˜åŒ–å™¨æ›´æ–°        â”‚
â”‚ â€¢ Train/Valåˆ†å‰² â”‚           â”‚   Head           â”‚          â”‚ â€¢ æ¢¯åº¦è£å‰ª          â”‚
â”‚                 â”‚           â”‚ â€¢ Decoder        â”‚          â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚                               â”‚
        â”‚                               â”‚                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Loss Functions      â”‚
                            â”‚                       â”‚
                            â”‚ â€¢ Dice Loss           â”‚
                            â”‚ â€¢ Focal Loss          â”‚
                            â”‚ â€¢ Uncertainty         â”‚
                            â”‚   Weighting           â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 æ–‡ä»¶ç»“æ„

```
MedSegDiff/
â”œâ”€â”€ guided_diffusion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bratsloader.py              # âœ… æ•°æ®åŠ è½½å™¨ï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ unet.py                     # âœ… UNetæ¨¡å‹ï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ losses.py                   # âœ… æŸå¤±å‡½æ•°ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ train_util.py               # âœ… è®­ç»ƒå¾ªç¯ï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ gaussian_diffusion.py       # âœ… æ‰©æ•£è¿‡ç¨‹ï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ script_util.py              # âœ… æ¨¡å‹åˆ›å»ºå·¥å…·ï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ dist_util.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ segmentation_train.py      # âœ… æ ‡å‡†è®­ç»ƒï¼ˆä¿®æ”¹ï¼‰
â”‚   â”œâ”€â”€ segmentation_train_staged.py  # âœ… é˜¶æ®µæ€§è®­ç»ƒï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ segmentation_sample_v2.py   # âœ… è¯„ä¼°è„šæœ¬ï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ MICCAI_BraTS2020_TrainingData/
â”‚       â”œâ”€â”€ name_mapping.csv        # âœ… ç—…ç†åˆ†çº§CSVï¼ˆæ–°å¢ï¼‰
â”‚       â”œâ”€â”€ BraTS20_Training_001/
â”‚       â”‚   â”œâ”€â”€ BraTS20_Training_001_t1.nii
â”‚       â”‚   â”œâ”€â”€ BraTS20_Training_001_t1ce.nii
â”‚       â”‚   â”œâ”€â”€ BraTS20_Training_001_t2.nii
â”‚       â”‚   â”œâ”€â”€ BraTS20_Training_001_flair.nii
â”‚       â”‚   â””â”€â”€ BraTS20_Training_001_seg.nii
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ README_COMPLETE_JOURNEY.md      # âœ… æœ¬æ–‡æ¡£ï¼ˆæ–°å¢ï¼‰
â””â”€â”€ requirements.txt                # âœ… ä¾èµ–ï¼ˆå·²æ›´æ–°ï¼‰
```

### 7.3 å…³é”®å‚æ•°é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | Stage 1 | Stage 2 | Stage 3 | è¯´æ˜ |
|-----|-------|---------|---------|---------|------|
| `--stage` | 1 | 1 | 2 | 3 | è®­ç»ƒé˜¶æ®µ |
| `--use_cls_head` | - | False | True | True | æ˜¯å¦å¯ç”¨åˆ†ç±»å¤´ |
| `--lr` | 1e-4 | 1e-4 | 1e-3 | 1e-5 | å­¦ä¹ ç‡ï¼ˆè‡ªåŠ¨è°ƒæ•´ï¼‰ |
| `--focal_gamma` | 2.0 | - | 2.0 | 2.0 | åˆ†ç±»Focal Loss Î³ |
| `--seg_focal_gamma` | 1.5 | - | - | 1.5 | åˆ†å‰²Focal Loss Î³ |
| `--seg_focal_lambda` | 0.5 | - | - | 0.5 | åˆ†å‰²Focal Lossæƒé‡ |
| `--diffusion_steps` | 1000 | 1000 | 1000 | 1000 | æ‰©æ•£æ­¥æ•° |
| `--batch_size` | 4 | 4 | 4 | 4 | æ‰¹æ¬¡å¤§å° |
| `--image_size` | 256 | 256 | 256 | 256 | å›¾åƒå¤§å° |
| `--num_channels` | 128 | 128 | 128 | 128 | æ¨¡å‹é€šé“æ•° |
| `--use_fp16` | False | False | False | False | æ··åˆç²¾åº¦ |
| `--save_interval` | 5000 | 5000 | 2000 | 1000 | ä¿å­˜é—´éš” |
| `--log_interval` | 100 | 100 | 50 | 50 | æ—¥å¿—é—´éš” |

---

## 8. ä½¿ç”¨æŒ‡å—

### 8.1 ç¯å¢ƒå‡†å¤‡

#### **1. å®‰è£…ä¾èµ–**ï¼š
```bash
pip install -r requirements.txt
```

**requirements.txt**ï¼ˆå…³é”®åº“ï¼‰ï¼š
```
torch>=1.10.0
torchvision>=0.11.0
numpy>=1.21.0
nibabel>=3.2.0
pandas>=1.3.0
scikit-learn>=0.24.0
matplotlib>=3.4.0
Pillow>=8.3.0
blobfile>=1.0.0
mpi4py>=3.1.0
```

#### **2. å‡†å¤‡æ•°æ®**ï¼š

**ç›®å½•ç»“æ„**ï¼š
```
Data/BraTS/MICCAI_BraTS2020_TrainingData/
â”œâ”€â”€ name_mapping.csv
â”œâ”€â”€ BraTS20_Training_001/
â”‚   â”œâ”€â”€ BraTS20_Training_001_t1.nii
â”‚   â”œâ”€â”€ BraTS20_Training_001_t1ce.nii
â”‚   â”œâ”€â”€ BraTS20_Training_001_t2.nii
â”‚   â”œâ”€â”€ BraTS20_Training_001_flair.nii
â”‚   â””â”€â”€ BraTS20_Training_001_seg.nii
â”œâ”€â”€ BraTS20_Training_002/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

**name_mapping.csvæ ¼å¼**ï¼š
```csv
BraTS_2020_subject_ID,Grade
BraTS20_Training_001,HGG
BraTS20_Training_002,LGG
BraTS20_Training_003,HGG
...
```

**åˆ›å»ºCSVç¤ºä¾‹**ï¼š
```python
# create_csv_example.py
import pandas as pd

data = []
for i in range(1, 370):  # 369 subjects
    subject_id = f"BraTS20_Training_{i:03d}"
    # å‡è®¾å‰76ä¸ªæ˜¯LGGï¼Œå…¶ä½™æ˜¯HGG
    grade = "LGG" if i <= 76 else "HGG"
    data.append({'BraTS_2020_subject_ID': subject_id, 'Grade': grade})

df = pd.DataFrame(data)
df.to_csv('Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv', 
          index=False)
print(f"Created CSV with {len(df)} entries")
```

### 8.2 å¿«é€Ÿå¼€å§‹ï¼šæ ‡å‡†è®­ç»ƒ

#### **å•é˜¶æ®µè®­ç»ƒï¼ˆç›´æ¥è”åˆä¼˜åŒ–ï¼‰**ï¼š

```bash
# è®­ç»ƒ
python scripts/segmentation_train.py \
  --data_name BRATS \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \
  --out_dir ./results \
  --image_size 256 \
  --num_channels 128 \
  --num_res_blocks 2 \
  --diffusion_steps 1000 \
  --noise_schedule linear \
  --lr 1e-4 \
  --batch_size 4 \
  --save_interval 5000 \
  --log_interval 100 \
  --use_fp16 False \
  --use_cls_head True \
  --focal_gamma 2.0 \
  --seg_focal_gamma 1.5 \
  --seg_focal_lambda 0.5

# è¯„ä¼°
python scripts/segmentation_sample_v2.py \
  --model_path ./results/savedmodel020000.pt \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --csv_path Data/BraTS/MICCAI_BraTS2020_TrainingData/name_mapping.csv \
  --data_name BRATS \
  --use_cls_head True \
  --num_eval_cases 20
```
---

## 10. æŠ€æœ¯æ€»ç»“ä¸å±•æœ›

### 10.1 æ ¸å¿ƒè´¡çŒ®

#### **1. æ¶æ„åˆ›æ–°**ï¼š
- âœ… é¦–æ¬¡åœ¨æ‰©æ•£æ¨¡å‹ä¸­é›†æˆå›¾åƒçº§åˆ†ç±»å¤´
- âœ… Bottleneck-basedåˆ†ç±»è®¾è®¡é¿å…å½±å“åˆ†å‰²
- âœ… ç»Ÿä¸€çš„forwardè¾“å‡ºï¼š`(seg_logits, cls_logits, calib_map)`

#### **2. æŸå¤±å‡½æ•°è®¾è®¡**ï¼š
- âœ… Focal Lossåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡
- âœ… åˆ†å‰²Dice + Focalç»„åˆæŸå¤±
- âœ… ä¸ç¡®å®šæ€§åŠ æƒè‡ªåŠ¨å¹³è¡¡ä»»åŠ¡
- âœ… æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–ï¼ˆclamp, gradient clippingï¼‰

#### **3. è®­ç»ƒç­–ç•¥**ï¼š
- âœ… é˜¶æ®µæ€§è®­ç»ƒé¿å…æ¢¯åº¦å†²çª
- âœ… å‚æ•°å†»ç»“/è§£å†»ç²¾ç»†æ§åˆ¶
- âœ… å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´
- âœ… è¯¦ç»†çš„æŒ‡æ ‡ç›‘æ§å’Œæ—¥å¿—

#### **4. å·¥ç¨‹å®è·µ**ï¼š
- âœ… Subject-levelæ•°æ®åˆ†å‰²é¿å…æ³„æ¼
- âœ… ç±»åˆ«æƒé‡è‡ªåŠ¨è®¡ç®—
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œä¿®å¤è„šæœ¬
- âœ… è¯¦å°½çš„æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

### 10.2 æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³

#### **æŒ‘æˆ˜1: å¤šä»»åŠ¡æ¢¯åº¦å†²çª**

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é˜¶æ®µæ€§è®­ç»ƒï¼ˆStage 1 â†’ 2 â†’ 3ï¼‰
- å‚æ•°å†»ç»“æ§åˆ¶æ¢¯åº¦æµ
- ä¸ç¡®å®šæ€§åŠ æƒè‡ªåŠ¨å¹³è¡¡

#### **æŒ‘æˆ˜2: æ•°å€¼ç¨³å®šæ€§**

**è§£å†³æ–¹æ¡ˆ**ï¼š
- Gradient clipping (max_norm=1.0)
- Loss clamping (0-10)
- Focal Lossç¨³å®šåŒ–ï¼ˆclamp p_tï¼‰
- å¢åŠ diffusion stepsï¼ˆâ‰¥1000ï¼‰

#### **æŒ‘æˆ˜3: ç±»åˆ«ä¸å¹³è¡¡**

**è§£å†³æ–¹æ¡ˆ**ï¼š
- Focal Loss (Î³=2.0)
- Class weighting (Î±_LGG=2.43, Î±_HGG=0.63)
- ç›‘æ§per-class F1 score

#### **æŒ‘æˆ˜4: è®­ç»ƒç¨³å®šæ€§**

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¦ç”¨FP16é¿å…ç±»å‹ä¸åŒ¹é…
- åŒ¹é…æ¨¡å‹é€šé“æ•°ï¼ˆ128ï¼‰
- ä¼˜åŒ–å™¨åªåŒ…å«å¯è®­ç»ƒå‚æ•°
- è¯¦ç»†çš„å‚æ•°éªŒè¯æ—¥å¿—

### 10.3 å·²çŸ¥é™åˆ¶

1. **è®­ç»ƒæ—¶é—´**ï¼šé˜¶æ®µæ€§è®­ç»ƒéœ€10-13å°æ—¶ï¼ˆæ¯”å•ä»»åŠ¡è®­ç»ƒé•¿ï¼‰
2. **åˆ†å‰²æ€§èƒ½**ï¼šè½»å¾®ä¸‹é™ï¼ˆDice 0.82 â†’ 0.76ï¼‰
3. **å†…å­˜æ¶ˆè€—**ï¼šåˆ†ç±»å¤´å¢åŠ çº¦1Må‚æ•°
4. **è¶…å‚æ•°æ•æ„Ÿ**ï¼šå­¦ä¹ ç‡ã€focal gammaéœ€è¦è°ƒä¼˜

### 10.4 æœªæ¥å·¥ä½œ
3. **æ›´å¤šæ•°æ®å¢å¼º**ï¼šåŒ»å­¦å›¾åƒä¸“ç”¨å¢å¼ºç­–ç•¥
4. **æ¨¡å‹å‹ç¼©**ï¼šå‡å°‘Highway networkå‚æ•°ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ˆè¿™ä¸ªå‚æ•°å¤ªå¤šå¤ªå“äººäº†


### 10.5 ç›¸å…³å·¥ä½œå¯¹æ¯”
ï¼ˆå®Œäº†ï¼Œè¿™é‡Œå¥½å¤šæ¶ˆèå®éªŒè¦åšå•Šï¼‰
| æ–¹æ³• | åˆ†å‰² | åˆ†ç±» | å¤šä»»åŠ¡å­¦ä¹  | æ‰©æ•£æ¨¡å‹ |
|-----|------|------|----------|---------|
| **UNet** | âœ… | âŒ | âŒ | âŒ |
| **ResNet** | âŒ | âœ… | âŒ | âŒ |
| **MT-UNet** | âœ… | âœ… | âœ… (ç®€å•æ±‚å’Œ) | âŒ |
| **MedSegDiff** | âœ… | âŒ | âŒ | âœ… |
| **MedSegDiff-V2 (Ours)** | âœ… | âœ… | âœ… (ä¸ç¡®å®šæ€§åŠ æƒ+é˜¶æ®µæ€§è®­ç»ƒ) | âœ… |

---

## é™„å½•A: å¸¸è§é—®é¢˜FAQ

### Q1: ä¸ºä»€ä¹ˆåˆ†å‰²æ€§èƒ½ä¸‹é™äº†ï¼Ÿ

**A**: è¿™æ˜¯å¤šä»»åŠ¡å­¦ä¹ çš„å›ºæœ‰trade-offï¼š
- å•ä»»åŠ¡MedSegDiffï¼šä¸“æ³¨åˆ†å‰²ï¼ŒDice=0.82
- å¤šä»»åŠ¡MedSegDiff-V2ï¼šå¹³è¡¡åˆ†å‰²å’Œåˆ†ç±»ï¼ŒDice=0.76
- é€šè¿‡é˜¶æ®µæ€§è®­ç»ƒï¼Œå·²å°†ä¸‹é™å¹…åº¦æ§åˆ¶åœ¨å¯æ¥å—èŒƒå›´ï¼ˆ-7%ï¼‰

**å»ºè®®**ï¼š
- å¦‚æœåªéœ€è¦åˆ†å‰²ï¼Œä½¿ç”¨åŸå§‹MedSegDiff
- å¦‚æœåŒæ—¶éœ€è¦åˆ†ç±»ï¼Œä½¿ç”¨MedSegDiff-V2

### Q2: è®­ç»ƒæ—¶å‡ºç°NaNæ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹é…ç½®ï¼š
```bash
# 1. å¢åŠ diffusion steps
--diffusion_steps 1000  # ä¸è¦ç”¨20, 50ç­‰å°å€¼

# 2. é™ä½å­¦ä¹ ç‡
--lr 1e-4  # ä¸è¦ç”¨1e-3

# 3. ç¦ç”¨FP16
--use_fp16 False

# 4. ä½¿ç”¨åŒ¹é…çš„é€šé“æ•°
--num_channels 128  # ä¸è¦ç”¨64
```


### Q4: å¦‚ä½•åªè®­ç»ƒåˆ†å‰²ä¸è®­ç»ƒåˆ†ç±»ï¼Ÿ

**A**: ä¸ä½¿ç”¨åˆ†ç±»å¤´ï¼š
```bash
python scripts/segmentation_train.py \
  --use_cls_head False \
  --data_name BRATS \
  --data_dir Data/BraTS/MICCAI_BraTS2020_TrainingData \
  --out_dir ./results \
  ...
```

### Q5: CSVæ–‡ä»¶æ ¼å¼é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: ç¡®ä¿CSVæ ¼å¼æ­£ç¡®ï¼š
```csv
BraTS_2020_subject_ID,Grade
BraTS20_Training_001,HGG
BraTS20_Training_002,LGG
```

**æ³¨æ„**ï¼š
- åˆ—åå¿…é¡»æ˜¯ `BraTS_2020_subject_ID` å’Œ `Grade`
- Subject IDæ ¼å¼å¿…é¡»åŒ¹é…æ–‡ä»¶å¤¹å
- Gradeåªèƒ½æ˜¯ `HGG` æˆ– `LGG`

### Q6: å¦‚ä½•è°ƒæ•´ç±»åˆ«æƒé‡ï¼Ÿ

**A**: ç±»åˆ«æƒé‡ä¼šè‡ªåŠ¨è®¡ç®—ï¼Œä½†å¯ä»¥æ‰‹åŠ¨è°ƒæ•´ï¼š
```python
# guided_diffusion/bratsloader.py
def _calculate_class_weights(self):
    # é»˜è®¤ï¼šé€†é¢‘ç‡åŠ æƒ
    weight_lgg = total / (2 * grade_counts[0])
    weight_hgg = total / (2 * grade_counts[1])
    
    # æ‰‹åŠ¨è°ƒæ•´ï¼ˆå¦‚æœéœ€è¦æ›´å¼ºè°ƒLGGï¼‰
    weight_lgg *= 1.5  # å¢å¤§LGGæƒé‡
    
    return torch.tensor([weight_lgg, weight_hgg])
```

---
